%!TEX root=./main.tex
\section{Experimental Validation}
\label{sec:experiments}
\subsection{Overview}
%We present a qualitative and quantitative evaluation of the proposed pipeline.
%We first present a quantitative evaluation with \textit{in-silico} data.
\SGG{We first assess the TRE error on a phantom and we then present a thorough evaluation with offline videos of real surgeries\footnote{All participants enrolled gave their written informed consent according to the approval of the ethical committee (IRB 2018-A03130-55).}, where we analyse three specific points of our pipeline: automatic keyframe selection, occlusion resistance in tracking and the use of new keyframes during tracking (see video material \url{https://bit.ly/2m9rGHH}.).}
We then describe our experiences with deploying and running the pipeline in the real clinical setting.
We emphasize that all previous works on real-time markerless monocular laparoscopic registration with pre-operative organ models have only been evaluated with pre-recorded video data.
Thus they have never made the step up to live tests in the OR and their robustness and practicality in the clinical setting have not been validated.

\subsection{Quantitative Evaluation with phantom}
\label{subsec:quantitative-evaluation-with-phantom}

\begin{figure}[tb]
  \centering
%  \includegraphics[width=0.4\columnwidth]{./figs/phantom.png}
  \includegraphics[width=0.49\columnwidth]{./figs/affineError.pdf}
  \includegraphics[width=0.49\columnwidth]{./figs/mechanicalError.pdf}
%  \includegraphics[width=0.4\columnwidth]{./figs/TRE_3Dmodels.pdf}
%  \caption{From left to right, the phantom used for the experiment, the TRE for the affine deformations and the mechanical deformations and the distribution of TRE on the surface of the 3D model.}
  \caption{TRE for affine (left) and mechanical (right) deformations. The deformation extent increases along the $x$-axis. The TRE is shown for different depths from the fundus.}
  \label{fig:TRE}
\end{figure}

\SGG{In our workshop paper~\cite{Collins2044} we presented a thorough evaluation of the initial registration step on a 3D printed uterus phantom and different synthetically deformed shapes.
%We synthetically deformed the CAD model to simulate a range of different pre-operative shapes of the uterus.
%To test the accuracy of the initial registration, we use an affine deformable model and we assessed the influence of the number of keyframes on the registration accuracy.
The results showed that the registration error decreases with the number of keyframes, and saturates approximately at \SI{1.4}{\milli\metre} for more than $8$ keyframes.
More generally, the error distribution tends to increase towards the cervix (\SI{2}{\milli\metre} for $15$ views up to \SI{8}{\milli\metre} for $2$ views) away from the uterus head as the uterus head is quite well constrained by the SfM point cloud as opposed to deeper regions near the cervix.
}

\SGG{In this section we present a new experiment to evaluate the Target Registration Error (TRE) of the the full registration pipeline (initial registration and tracking) with a realistic latex phantom (Limbs and Things Inc. Model XX). We collected $\termName{N}{gt}=200$ images of the phantom, from which we computed a 3D surface model using MVS. We used this model as the pre-operative model, and we
%We then run our reconstruction pipeline on a subset of $\termName{N}{s}=15$ images.% and we scaled and aligned the obtained MVS model to the GT model to get a metric reconstruction in the same reference frame.
synthetically deformed it to simulate different preoperative states with two kinds of deformations: isovolumetric affine deformations, and mechanical deformations~\cite{Collins2044} that simulate bending of the organ using a quadratic deformation law along a principal axis. 
We generated $10$ examples of each type, progressively increasing the amount of deformation magnitude. For each deformation $\Theta(\cdot)$, we applied our registration pipeline, using $\termName{N}{s}=15$ images for the initial registration step and $\termName{N}{gt}-\termName{N}{s}$ images for tracking.
%non-rigid registration with the MVS model and we ran our tracker on the remaining $\termName{N}{gt}-\termName{N}{s}$ images.
We discretized the model  with a grid of $100\times100\times100$ voxels and, denoting $\mathcal{V}$ the set of voxels in the interior of the uterus, we computed TRE for each voxel $\vet{q} \in \mathcal{V}$ in each image as $\| f( \Theta(\vet{q}),\mathbf{x}_t) -  \mat{P}_{\text{gt}}(\vet{q}) \|$, where $\vet{x}_t$ are the model registration parameters estimated by our pipeline and $\mat{P}_{\text{gt}}$ is the ground truth pose provided by the MVS reconstruction. TRE varies as a function of both the depth of the target and the amount of organ deformation. We visualize the trends in \fig{fig:TRE}. We show TRE averaged over all the tracked frames for voxels at 30 different depths from the head, ranging from 0 mm to 100 mm (approximately the cervix). TRE in the near fundus region (depth $<$ranges from $\SI{1.8}{\milli\metre}$ to $\SI{2.1}{\milli\metre}$ for the most severe affine deformation.
The error increases towards the cervix, from $\SI{6.7}{\milli\metre}$ up to $\SI{15.6}{\milli\metre}$. The increase in TRE with depth from the uterus head is normal and expected because of the increasing distance from the visible surface. The low registration error indicates the pipeline is sufficiently accurate for AR guidance of structures in the uterus fundus region. 
}

%\subsection{Quantitative Evaluation with Simulation experiments}
%\label{sec:experiments_Simulation}
%
%We present an evaluation of our initial registration stage using a $90 \times 60 \times 60~\si{\cubic\milli\metre}$  3D printed rigid uterus phantom where its preoperative shape deformation is simulated in software.
%\fig{fig:phantom}(a)) shows the phantom body with a printed realistic texture extracted from images of a human uterus and placed inside a pelvic trainer (\fig{fig:phantom}(b)) to simulate the laparoscopic conditions.
%The exploratory video is taken with a $\SI{10}{\milli\metre}$ Karl Storz HD laparoscope fixed to the pelvic trainer through a flexible arm.
%To simulate the rotation of the cannula as in real surgery, we rotated the model about the cervix and we acquired $K=20$ keyframes (\fig{fig:reconstruction}).
%% We introduced the phantom into a pelvic trainer (\fig{fig:phantom}(b)) to simulate the laparoscopic conditions, using a $\SI{10}{\milli\metre}$ Karl Storz HD laparoscope fixed to the pelvic trainer through a flexible arm.
%% We performed the exploratory video by rotating the model about the cervix similarly to the motion experienced in real surgery by rotating the cannula, keeping $K=20$ keyframes (\fig{fig:reconstruction}).
%
%\begin{figure}[t]
%  \centering
%  \includegraphics[width=0.5\textwidth]{./figs/phantom.pdf}
%\caption{(a) Uterus CAD model. (b) 3D printed phantom inside a pelvic trainer with a \SI{10}{\milli\metre} Karl Storz HD laparoscope. (c) Laparoscopic images of the phantom.}
%\label{fig:phantom}
%\end{figure}
%
%
%\begin{figure*}[htb]
%  \centering
%  \includegraphics[width=0.9\textwidth]{./figs/markupWithDefSimCompressed.pdf}
%\caption{Left: Exploratory keyframes with occluding contour fragments. Middle: Sparse 3D cloud from SfM. Right: The grid of control points  with the 3D pre-operative model (up) and an example of the generated synthetic deformations (bottom).}
%\label{fig:reconstruction}
%\end{figure*}
%
%We synthetically deformed the CAD model to simulate a range of different pre-operative shapes of the uterus.
%We employed Thin-Plate Spline (TPS) to represent the volumetric deformation model.
%We uniformly sampled the CAD model with a grid of $2\times 3 \times 3$ control points.
%% To this end we used a volumetric deformation model represented with the Thin-Plate Spline (TPS).
%% The TPS was defined by a set of $2\times 3 \times 3$ control points uniformly spaced over the CAD model.
%\fig{fig:reconstruction} shows the non-linear volumetric deformations that we could generate by perturbing the control point set.
%% By perturbing the control point grid we could create non-linear volumetric deformations (see \fig{fig:reconstruction}, right).
%This enabled us to create many pre-operative deformations: first the reference control points were transformed with a random global affine transformation and then they underwent a random displacement of standard deviation $\sigma_p=\SI{10}{\milli\metre}$.
%% We generated many pre-operative deformations first applying a random global affine transform to the reference control points followed by a random displacement of standard deviation $\sigma_p=\SI{10}{\milli\metre}$.
%We define the TPS as $\wTPS(\cdot,\thetaTPS)$, where $\thetaTPS$ holds the control centers (\ie $18\times 3$ parameters in total).
%
%%
%%\begin{figure}[htb]
%  %\centering
%  %\includegraphics[width=0.3\textwidth]{./figsSimulation/deformSimulation.pdf}
%%\caption{Up: Intra-operative phantom surface, overlaid with the TPS grid of control points. Down: Several synthetic deformations obtained with the TPS.}
%%\label{fig:uterusDeformations}
%%\end{figure}
%To test the accuracy of the initial registration, we use an affine deformable model.
%This was chosen because the synthetic deformation we induce with the TPS model cannot be exactly approximated by the deformation model.
%We used this to test our system's robustness to unmodelled deformation modes, which likely occur when dealing in real conditions with real deformations.
%To assess the influence of the number of keyframes on the registration accuracy, we randomly selected sets of $\{2,3,4,5,6,7,10,15\}$ images from the initial $K=20$ keyframes.
%For each of them we generated $50$ random deformations with the TPS.
%% Our quantitative experiment tested the influence of the number of keyframes on the registration accuracy. We randomly selected sets of $\{2,3,4,5,6,7,10,15\}$ images from the $K=20$ keyframes. For each set we generated $50$ random deformations with the TPS.
%\fig{fig:results}(a) shows the marked contours from $3$ different keyframe overlaid with the shape of three deformed models.
%We measured registration error in 3D by discretizing the internal volume of the uterus model with a grid of $100\times 100 \times 100$ voxels. We defined the registration error $\epsilon_r$ as follows:
%%\begin{figure}[htb]
%%  \centering
%%  \includegraphics[width=0.35\textwidth]{./figs/initial_.pdf}
%%\caption{(a), (b) and (c) show the initial projection of three different pre-operative models (one model per column) in three different views (one view per row) using the $z$-buffer. We overlay on top the contours from each view given by the user.}
%%\label{fig:InitialSolutions}
%%\end{figure}
%\begin{figure*}[htbp]
%  \centering
%  \includegraphics[width=0.95\textwidth]{./figs/errors.pdf}
%\caption{(a) Each column shows the initial projection of a different pre-operative model in $3$ views (one per row). The red curves show the contours marked by the user. (b) Registration error vs. number of views. (c) Number of iterations vs. number of views. (d) Mean surface registration error for $2$, $4$ and $15$ views.}
%\label{fig:results}
%\end{figure*}
%
%\begin{equation}
%  \label{eq:regerror}
%  \epsilon^2_r=\frac{1}{N_{\mathcal{V}}}\sum_{\mathbf{q}\in \mathcal{V}} \|f(\wTPS(\mathbf{q};\thetaTPS),t_0;\theta)-\mathbf{q}\|^2,
%\end{equation}
%where $\mathcal{V}$ is the set of voxels in the interior of the uterus volume.
%We report in \fig{fig:results}(b) the mean registration error as a function of the number of the available keyframes.
%In \fig{fig:results}(c) we show the mean number of iterations our method requires to reach convergence.
%% \fig{fig:results}(b) shows the mean registration error as a function of the number of available keyframes. \fig{fig:results}(c) shows the mean number of iterations our method requires to reach convergence.
%\fig{fig:results}(d) shows the distribution of registration error over the uterus surface using a heatmap. The registration error clearly decreases with the number of views, and saturates approximately at \SI{1.4}{\milli\metre} for more than $8$ views. With $2$ views the registration error distribution in \fig{fig:results}(d) tends to increase towards the cervix, away from the uterus head. This makes sense as the uterus head is quite well constrained by the SfM pointcloud as opposed to the area near the cervix.
%By contrast, with more views we have more constraints from the silhouette contours, as these reveal the boundaries of the uterus from a greater range of viewpoints, and the result is a significant improvement in registration. This experiment provides strong evidence for the value in using multiple viewpoints to constrain the registration problem, compared with prior works that uses only one viewpoint. In addition we that more views significantly reduce the number of iterations required for convergence.

\subsection{Evaluation with Human Uteri in Pre-recorded Videos}
\label{sec:orbslam}
In this section we present results on videos recorded during laparoscopic surgery.
We test accuracy, computational complexity and the influence of the keypoint detector in our real-time organ tracking by detection system, named RT-OTD in the experiments.
\SGG{We demonstrate the robustness of our model-based approach \wrt classic SLAM approaches~\cite{orbslam_laparo} and we show augmentation results, completing the AR pipeline}
%We compare it with ORB-SLAM, state-of-the-art for SLAM-based tracking in laparoscopic videos~\cite{orbslam_laparo}.
%We show augmentation results, completing the AR pipeline.


\subsubsection{Automatic Keyframe Selection for 3D Reconstruction}
\label{subsec:automatic-keyframe-selection}
\SG{We compare the automatic keyframe selection method \emph{auto} to results obtained by sampling the exploratory video ($\sim \SI{60}{\second}$) to obtain the $15$ keyframes, with different sampling methods: \emph{equally} samples the video uniformly, \emph{beginning}, \emph{middle} and \emph{end} take a keyframe every $t=\SI{2}{\second}$ from the beginning, around the middle and towards the end of the video, respectively.
We then proceed with reconstruction and tracking for each strategy.
\fig{fig:3DresultsKframeSelection} shows the 3D models.
Qualitatively, \emph{auto} gave a better model as all the others have many holes.
This is explained by 1) some selected keyframes are blurry and 2) there are many similar keyframes, thus preventing a complete coverage of the full organ shape.}

\SG{We recorded another video ($\sim \SI{60}{\second}$) in order to test the tracking  using the 3D models obtained from each method.
The video was purposely challenging, with the camera looking at parts of the uterus that were not completely reconstructed in any of the models, and no additional keyframes added during tracking.
\emph{auto} was able to track the largest number of frames ($\SI{55.18}{\percent}$), followed by \emph{equally} ($\SI{53.33}{\percent}$), \emph{middle} ($\SI{52.48}{\percent}$), \emph{beginning} ($\SI{46.14}{\percent}$) and \emph{end} ($\SI{31.08}{\percent}$).
\fig{fig:statsKframeSelection} reports tracking statistics.
In general, \emph{auto} recovers a larger number of valid matches, both \wrt all the keyframes in the database (\fig{fig:statsKframeSelection}.a) and the winning keyframe (\fig{fig:statsKframeSelection}.b) and computes the pose with a larger number of inliers.
The reprojection error is also slightly better than all the other methods, especially considering the higher number of inliers over which it is computed.
Overall, automatic keyframe selection improves the quality of reconstruction and tracking.
Any sampling method is always potentially affected by motion blur; guiding the acquisition of the keyframe reduces the risk.}
%$\SI{46.14}{\percent}$ $\SI{52.48}{\percent}$ $\SI{31.08}{\percent}$ $\SI{53.33}{\percent}$ $\SI{55.18}{\percent}$

\subsubsection{Comparison of Different Tracking Keypoints}
\label{sec:siftvssurf}
%In this section we analyse the benefits of using SIFT features over SURF within the tracking stage. %SIFT features offer, in general, a more reliable and robust choice for feature matching but the major limitation is its computational time, which often prevents them from being used in real-time applications.
%We used a SIFT GPU implementation~\cite{Griwodz2018Popsift} which provides real-time performances with full HD images.
We tested our tracking method RT-OTD using PopSift~\cite{Griwodz2018Popsift} and the SURF-GPU from OpenCV on two videos.  
\begin{table}[]
\centering
\begin{tabular}{rlcccc}
\multicolumn{1}{c}{\# frames}              & \multicolumn{1}{c}{} & \# poses & \# matches & \begin{tabular}[c]{@{}c@{}}\# matches \\ winner\end{tabular} & \# inliers                 \\ \hline \hline
\multicolumn{1}{|r}{\multirow{2}{*}{$5000$}} & SURF                 & $4569$     & $406.16$     & $46.94 $                                                       & \multicolumn{1}{c|}{$32.93$} \\
\multicolumn{1}{|r}{}                      & SIFT                 & $4975$     & $388.06$     & $52.31$                                                        & \multicolumn{1}{c|}{$44.86$} \\ \hline
\multicolumn{1}{|r}{\multirow{2}{*}{$3029$}} & SURF                 & $2713$     & $296.81$     & $42.60$                                                        & \multicolumn{1}{c|}{$27.74$} \\
\multicolumn{1}{|r}{}                      & SIFT                 & $3029$     & $294.00$     & $64.87$                                                        & \multicolumn{1}{c|}{$52.81$} \\ \hline
\end{tabular}
\caption{Comparison in number of poses and matches between SIFT and SURF for two videos.}
\label{tab:matches}
\vspace{-2mm}
\end{table}
We see from \tab{tab:matches} that RT-OTD using SIFT establishes more camera poses (\SI{99.5}{\percent} and \SI{100}{\percent} of the frames) compared to SURF (\SI{91.2}{\percent} and \SI{89.6}{\percent}).
Despite the fact that SURF recovers, on average, more matches between $\mathcal{F}$ and $\mathcal{G}$, SIFT has a higher discriminative power in selecting the winning keyframe $i^*$.
As the table shows, the winning keyframe has, on average, more available matches from which RANSAC can sample to compute pose.
This also results in more inliers found to support the computed pose.
\begin{figure}[t]
  \centering
%  \includegraphics[width=0.4\textwidth]{./figs/Stability_Features.pdf}
  \includegraphics[width=0.45\textwidth]{./figs/Stability_Features2.pdf}
\caption{Comparing SURF and SIFT on $5000$ video frames.}
\label{fig:SurfVsSift}
\vspace{-5mm}
\end{figure}

\begin{figure}[ht]
  \centering
%  \includegraphics[width=0.835\textwidth]{./figs/frames_aug_new.pdf}
  \includegraphics[width=0.99\columnwidth]{./figs/frames_aug_new.pdf}
\caption{(a) The 3D preoperative model of the uterus with the  myoma as reconstructed from the MR, (b) the preoperative model registered with the intraoperative model obtained from the MVS reconstruction (c) some of the keyframes used for MVS with the AR augmentation  (d) some frames of the video with the myoma shown as image overlay.}
\label{fig:myomas}
\vspace{-5mm}
\end{figure}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.71\textwidth]{./figs/ARinOR.pdf}
\caption{Patient 1: (a) MR image showing one myoma, (b) myoma augmentation, (c) resection of the myoma, (d) deployment of our AR system in the OR. Patient 2: (e) MR image showing three myomas, (f,g,h) myoma augmentation. Patient 3: (i) two myomas segmented from the MR images, (j,k,m) augmentation of the myomas and the uterine cavity.}
\label{fig:realOR}
\end{figure*}

%\fig{fig:numMatches} shows the numbers of matches and inliers for both approaches for the first video sequence, where we see that all along the frames the number of matches used for computing the pose are always higher for SIFT. 
%In order to evaluate the quality of the estimated poses, 
We show both pose components in \fig{fig:SurfVsSift} for the $5000$ frame video.
SIFT provides a more stable estimate for both as there are much fewer spikes in the estimates (spikes are typically incorrect estimates).
This translates to a more stable motion estimate, improving overall AR quality with SIFT\@.
% The increased stability requires also less new keyframes to be added, thus, in turn, improving the overall computational time as less descriptor comparisons are needed.
The two videos and further results are shown in the video material. 

%\subsubsection{Tracking Accuracy and Comparison with ORB-SLAM}
\subsubsection{Tracking Accuracy and Comparison with SLAM}
We evaluate our tracking stage using three human uteri captured before hysterectomy.
They can be seen in \fig{fig:hister} and we refer to them as $U_1$, $U_2$ and $U_3$.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.4\textwidth]{./figs/snapshotsCompressed.pdf}
\caption{The 3D models from MVS, the coagulation marks and the registered models.}
\label{fig:hister}
\vspace{-5mm}
\end{figure}
Each video includes around $500$ frames showing image motion of the uterus due to motion induced by the cannula and camera motion.
The uterus intraoperative surface is obtained with an exploratory video with $15$ keyframes.
The uterus body was marked by the surgeon with a bipolar grasper in $12-15$ different locations.
% To obtain Ground-Truth (GT) camera pose the surgeon marked the uterus with a coagulation instrument at $12-15$  locations over the uterus body. 
This enabled us to generate Ground-Truth (GT) camera poses by tracking the obtained set of small marked regions ($\sim\SI{3}{\milli\metre}$ in diameter).
% This produced a set of small regions approximately $\SI{3}{\milli\metre}$ in diameter which could be accurately tracked. 
We show examples of these markers in \fig{fig:hister}. 
The marks were tracked using a small patch surrounding the image position of each marker, and fitted using a 2D affine transform. 
We verified all tracks and reinitialized them if they were lost.
We then computed the marks' 3D positions and the uterus 3D poses in each frame using SfM\@.
If fewer than four marks were visible in a frame we considered that the GT pose could not be estimated for that frame.
We masked each mark so that the methods under comparison could not exploit the artificial texture each mark introduces. 
We computed the optimal scale factor for each method \wrt GT\@.
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.78\columnwidth]{./figs/Accuracy_errors.pdf}
\caption{Rotation error, position error and camera trajectory, for $U_1$, $U_2$ and $U_3$ from top to bottom.}
\label{fig:hister_results}
 \vspace{-3mm}
\end{figure}
\SGG{In~\fig{fig:hister_results} we summarize the results of RT-OTD using \SG{SIFT} features.}
%We summarize the results of RT-OTD, using \SG{SIFT} features, against ORB-SLAM in \fig{fig:hister_results} for the three uteri.
The rotation error (in degrees) and position error (in mm) are computed as the Euclidean norm between GT and estimates.
There are gaps in the graphs when GT is not available.
\SGG{The results with RT-OTD are very accurate  and stable in all cases, with an average position error of $\SI{2}{\milli\metre}$  and an average rotation error of $\ang{3}$.}
%We also show the camera 3D trajectories estimated by RT-OTD and ORB-SLAM.
\SGG{\fig{fig:hister_results} also shows the tracking performed with a state-of-the-art open-source implementation of SLAM, ORB-SLAM.}
While it tracks a large portion of the first sequence, it quickly degenerates and loses the track for the other two cases.
This is due to the fact that SLAM builds a model with points from both the moving uterus and the background, violating the rigidity assumption.
It is also affected by motion blur and the lack of matches with the map.
%The results with RT-OTD are very accurate  and stable in all cases, with an average position error of $\SI{2}{\milli\metre}$  and an average rotation error of $\ang{3}$.


\subsubsection{Robustness Test for Tracking}
\label{subsec:robustTracking}
\SG{Since it is hard to obtain GT data in the laparoscopic setting, we evaluated tracking robustness \wrt occlusions from the keyframes used for 3D reconstruction, for which a reference pose is available from SfM, forming the GT. 
Using the data collected from $5$ patients, we simulated occlusions and compared the pose computed by the tracker to the GT. 
In a first experiment, we used the known mask of the uterus to add a black occluder starting from the external contour and towards the center of the uterus.
We gradually increased the size of the occluder and launched tracking on the keyframe.
\fig{fig:statsKframeSelection}.e shows the overall percentage of frames that could be tracked against the occlusion ratio.
The occlusion ratio is computed from the number of pixels of the occluder and of the uterus.
The graph shows that tracking copes with up to $\SI{60}{\percent}$ occlusion, where $\sim\SI{80}{\percent}$ of the frames are tracked.
For the same experiment, \fig{fig:statsKframeSelection}.f shows that the pose rotational error is below $\sim\ang{1}$ for up to $\SI{50}{\percent}$ occlusion.}
\SG{In a second experiment we simulated the presence of tools or bleeding covering the appearance of the uterus by growing black spots randomly distributed on the the uterus.
\fig{fig:statsKframeSelection}.g shows that tracking successfully estimates pose for up to $\SI{60}{\percent}$ occlusion, where $\sim\SI{90}{\percent}$ of the frames are tracked, with rotational error below or of the order of $\sim\ang{1}$.}
\SGG{We refer the reader to \url{https://bit.ly/2WKO6hI}) for quantitative and qualitative evaluation of the robustness.}

\begin{figure}
  \centering
  \includegraphics[width=0.12\columnwidth]{./figs/kframeSelection/beginning.png}\hspace{0.5mm}
  \includegraphics[width=0.12\columnwidth]{./figs/kframeSelection/middle.png}\hspace{0.5mm}
  \includegraphics[width=0.12\columnwidth]{./figs/kframeSelection/end.png}\hspace{0.5mm}
  \includegraphics[width=0.12\columnwidth]{./figs/kframeSelection/equally.png}\hspace{0.5mm}
  \includegraphics[width=0.15\columnwidth]{./figs/kframeSelection/auto.png}
  \caption{From left to right, the 3D models generated using the keyframes extracted with \emph{beginning}, \emph{middle}, \emph{end}, \emph{equally}, \emph{auto}.}
  \label{fig:3DresultsKframeSelection}
\end{figure}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{./figs/ResultsC.pdf}
\caption{(a,b,c,d) Tracking statistics (number of matches, inliers, and pose errors) for each keyframe sampling method.% ( In each graph, the box plot represents the statistical distribution of the data, the blue box represents the second and third quartiles with a notch on the median value, while the whiskers represents the max and min values. 
(e,f,g,h) From left to right, histogram of successfully tracked frames and rotation error of estimated pose, for both experiments.}
\label{fig:statsKframeSelection}
\end{figure*}

%\begin{figure}
%  \centering
%  \begin{tabular}{cc}
%  \includegraphics[width=0.20\textwidth]{./figs/kframeSelection/matching.pdf}&
% \includegraphics[width=0.20\textwidth]{./figs/kframeSelection/matcheswinningKF.pdf}\\
%  \includegraphics[width=0.20\textwidth]{./figs/kframeSelection/matching.pdf}&
%  \includegraphics[width=0.20\textwidth]{./figs/kframeSelection/rmsePnP.pdf}
%  \end{tabular}
%\caption{In each graph, the box plot represents the statistical distribution of the data, the blue box represents the second and third quartiles with a notch on the median value, while the whiskers represents the max and min values.}
%\label{fig:statsKframeSelection}
%\end{figure}

%\begin{figure}
%  \centering
%%  \include{./figs/stresstest/histo_all}
%  \begin{tabular}{cc}
%  \includegraphics[width=0.20\textwidth]{./figs/stresstest/trackingHisto.pdf}&
% \includegraphics[width=0.20\textwidth]{./figs/stresstest/rotError.pdf}\\
%  \includegraphics[width=0.20\textwidth]{./figs/stresstest/trackingHistoPois.pdf}&
% \includegraphics[width=0.20\textwidth]{./figs/stresstest/rotErrorPois.pdf}
% \end{tabular}
%  \caption{From left to right, histogram of successfully tracked frames and rotation error of estimated pose, for both experiments.}
% \label{fig:stressTest}
%\end{figure}


\subsubsection{Additional Keyframes During Tracking}
\label{sec:exp_additionalkeyframes}
\SG{We present results to show the impact of adding keyframes during tracking.
We refer the reader to the video material to observe the qualitative impact on the stability of pose and quality of AR (\url{https://bit.ly/34wEhXu}).
\tab{tab:addKf} shows the number of tracked keyframes for three videos with and without additional frames.
Importantly, even a single new keyframe can greatly improve the number of tracked frames, as in video 20190801 in which it almost doubles the number of tracked frames.
In general however, the number of tracked frames does not vary considerably but the tracking is much more stable.}


\begin{table}[]
  \begin{tabular}{ll|l|ll}
    video id & \# frames & \begin{tabular}[c]{@{}l@{}}\% tracked w/o \\ additional keyframes\end{tabular} & \begin{tabular}[c]{@{}l@{}}added \\ keyframes\end{tabular} & \% tracked \\
    \hline
    \hline
    20190801 & $1943$      & $\SI{55.58}{\percent}$                             & $1$              & $\SI{99.90}{\percent}$     \\
    20190612 & $5119$      &  $\SI{95.64}{\percent}$                                   & $8$                & $\SI{97.15}{\percent}$     \\
    20190724 & $2444$          & $\SI{87.52}{\percent}$                                    & $4$               & $\SI{87.96}{\percent}$
  \end{tabular}
  \caption{Number of tracked frames with and without adding new keyframes during tracking.}
  \label{tab:addKf}
\end{table}



\subsubsection{Computational Times}
Our hardware is composed of a desktop PC running Linux Ubuntu with an Intel Core i$7$-$5960$X CPU running at \SI{3.00}{\giga\hertz} with \SI{16}{\giga\byte} of RAM and an NVIDIA GeForce GTX $980$ Ti graphics card.
On average, tracking takes \SI{16.35}{\milli\second} with a full HD $1920\times1080$ image.
SIFT takes \SI{11.1}{\milli\second}, descriptor matching between $\mathcal{F}$ and $\mathcal{G}$ takes \SI{3.21}{\milli\second}  and pose estimation takes \SI{0.18}{\milli\second}.

% \subsubsection{Comparison with ORB SLAM}
% \label{sec:orbslam}
% We have compared with ORB-SLAM2, which provides state-of-the-art results monocular SLAM results on laparoscopic videos \cite{orbslam_laparo}. As discussed in \sect{sec:sotaTracking} a scene that consists of independently moving structures such as the uterus will cause difficulty for current monocular SLAM methods, because they will amalgamates features from different structures into one map. This can be extremely problematic when there is slow relative motion between the structures. \fig{fig:orbAR} shows an example of this problem. On the left, the ORB features used for tracking are shown, covering the uterus (foreground) and surrounding peritoneum. When the uterus is then moved significantly \wrt the surroundings, the pose is computed wrongly, because background features have a strong influence on the estimated pose.


% % \begin{figure}
% % \centering
% % \input{figs/numInliers.tex}
% % \caption{ My first matlab2tikz figure }
% % \label{fig:myfirstfig}
% % \end{figure}

% \begin{figure*}
%   \centering
%   \includegraphics[width=0.32\textwidth]{./figs/numMatches.pdf}
%   \includegraphics[width=0.32\textwidth]{./figs/numMatchesWinning.pdf}
%   \includegraphics[width=0.32\textwidth]{./figs/numInliers.pdf}
% \caption{Comparison of the number of matches found using SURF (red) and SIFT (blue) over a sequence of $5000$ frames. The leftmost image shows the number of matches between $\mathcal{F}$ and $\mathcal{G}$ after the LRT, the center image shows the number of matches of the winning kframe and the rightmost image shows the number of inliers that supports the estimated pose.}
% \label{fig:numMatches}
% \end{figure*}

% \begin{figure*}
%   \centering
%   \includegraphics[width=0.32\textwidth]{./figs/translationX.pdf}
%   \includegraphics[width=0.32\textwidth]{./figs/translationY.pdf}
%   \includegraphics[width=0.32\textwidth]{./figs/translationZ.pdf}
%   \includegraphics[width=0.32\textwidth]{./figs/rotationX.pdf}
%   \includegraphics[width=0.32\textwidth]{./figs/rotationY.pdf}
%   \includegraphics[width=0.32\textwidth]{./figs/rotationZ.pdf}
% \caption{Estimated pose components (top, translation $t_x$, $t_y$ and $t_z$, bottom rotation angles $r_x$, $r_y$ and $r_z$) using SURF (red) and SIFT (PopSift implementation) features over a sequence of $5000$ frames. It can be noted that SIFT enable a more stable and less jittered estimation of the components.}
% \label{fig:SurfVsSift}
% \end{figure*}




 

% \begin{figure}
%   \centering
%   \includegraphics[width=0.45\columnwidth]{./figs/frame0001Features.png}
%   \includegraphics[width=0.45\columnwidth]{./figs/frame0107Features.png}
%   \includegraphics[width=0.45\columnwidth]{./figs/frame001.png}
%   \includegraphics[width=0.45\columnwidth]{./figs/frame107.png}  
% \caption{An example of tracking issue of SLAM approaches: even if the model is correctly registered and tracked (left), it can happen that due to the movements of the organ and the occlusions the tracker tracks the camera pose \wrt the background thus affecting the AR visualization (right). }
% \label{fig:orbAR}
% \end{figure}


% % The input models used in the presented experiments are generated from T2 weighted MRI with segmentation done semi-automatically using MITK~\cite{Wolf_themedical}.
% % The deformation models that we experiment with are tetrahedral Finite Element Models (FEMs) built with a 3D vertex grid (\SI{6}{\milli\metre} spacing) cropped to the organ.
% % Therefore, $\vet{x}_t$ holds the unknown 3D positions of the FEM's vertices in laparoscope coordinates.
% % Trilinear interpolation was used to compute $f(\vet{p};\vet{x}_t)$.
% % For $\Einternal$ the Saint Venant-Kirchoff strain energy is used, with patient-generic values for the Young's modulus $E$ and Poisson's ratio $\nu$.
% These were for healthy kidney tissue $E=\SI{7}{\kilo\pascal}$, $\nu=0.43$~\cite{Li_TIP08}, 
% % For healthy uterus tissue we use $E=\SI{96}{\kilo\pascal}$, $\nu=0.45$~\cite{uterusModel} and $E=\SI{532}{\kilo\pascal}$, $\nu=0.48$ for myomas~\cite{uterusModel}.
%Note that in the registration problem there is always a balancing weight between the internal energy and energy coming from image cues (which have no real physical meaning).
%Therefore only the relative values of $E$ are important to us (with respect to the balancing weight), rather than their absolute values.

\subsubsection{Augmentation Results}
\fig{fig:myomas}(a) shows the preoperative data of a patient whose uterus contains a myoma (in green) of a size of $\SI{11.3}{\milli\metre}\times\SI{22.9}{\milli\metre}\times\SI{17.5}{\milli\metre}$.
The preoperative mesh of the uterus has $2488$ vertices and $4972$ faces.
From the exploratory video $15$ keyframes were extracted to perform MVS reconstruction, obtaining a 3D model of $1760$ vertices.
\fig{fig:myomas}(b) shows the successful alignment between the preoperative data and the MVS reconstruction.
The cost function \eq{eq:totalCost} was optimized in $6$ iterations in approximately \SI{21}{\second}.
In \fig{fig:myomas}(c) we show four keyframes used for the reconstruction together with an overlay of the uterus surface registered to each frame.
Qualitatively we see that the 3D preoperative model aligns well to the image of the uterus.
Finally, \fig{fig:myomas}(d) reports the visual augmentation of the myoma in some frames of a video recorded during surgery, clearly showing that the uterus is tracked well. 
The full video is provided as supplemental material along with videos from other patients.


% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=\textwidth]{./figs/frames_aug.pdf}
% \caption{(a) The MRI preoperative data, (b) the 3D preoperative model of the uterus with the two myomas as reconstructed from the MRI, (c) frame from the video feed with the mesh model of the uterus from the MRI overlaid in each image and (d) its AR augmentation with the myomas shown as image overlay.}
% \label{fig:myomas}
% \end{figure*}
% Figures \fig{fig:myomas}(a) and \fig{fig:myomas}(b) show the preoperative data of a patient with two myomas. One myoma was large, with a diameter of \SI{121}{\milli\metre}, and the other medium sized with a diameter of \SI{52}{\milli\metre}. The preoperative mesh of the uterus has $6210$ vertices.
% %$\mathcal{M}$ was constructed by performing an interactive segmentation of the MRI using MITK~\cite{Wolf_themedical}, followed by meshing with marching cubes, two iterations of Laplacian mesh smoothing and then mesh decimation with quadratic edge collapse~\cite{conf/siggraph/GarlandH97} to give a mesh of $6210$ vertices. 
% The laparoscopic video was recorded during the patient's myomectomy and we performed the registration and augmentation off-line after surgery on a \SI{50}{\second} video clip before resection began (including the 20 second exploratory phase). We used $10$ keyframes to perform the SfM from the exploratory phase, and this computed correspondences for $321$ image features located on the uterus. The cost function in \eq{eq:totalCost} was optimised in $6$ iterations in approximately \SI{15}{\second}. % with our \CC implementation.
% % This took approximately \SI{15}{\second} to compute using an unoptimised Matlab implementation with a \CC $z$-buffer implementation.
% %The registration for the frames after the exploratory video was achieved using the method described in \sect{sec:registration}, which processed each frame in approximately \SI{37.0}{\milli\second} (or \SI{27}{fps}).
%  We show four frames from the clip in \fig{fig:myomas}(c), together with an overlay of the uterus surface registered to each frame. Its silhouette boundary is displayed in red. 
% %We show with a red contour the silhouette boundary of $\mathcal{M}$ in each frame.
% Qualitatively we see that the red contours align well to the real image contours shows clearly that the registration track the uterus well over the sequence. In \fig{fig:myomas}(d) we show the visual augmentation of the myomas in each frame (the larger myoma is in green and the smaller is in blue). The surgeon who conducted the myomectomy reported that localising the small myoma was difficult, and took them approximately $15$ minutes. After surgery they inspected our augmentations and confirmed that both myomas appeared to be localised well by the system. %This promising result indicates the potential benefit for using our AR system during surgery.
% %\begin{figure*}[ht]
% %  \centering
% %  \includegraphics[width=0.75\textwidth]{./figs/invivoHumanUteri/qualitative.pdf}
% %  \caption{Registration between the uterus in a pre-operative MRI to intra-operative laparoscopic images, and visual augmentation of two hidden myomas.
% %The top row shows $\mathcal{M}$ (the mesh model of the uterus from the MRI ) overlaid in each image.
% %The red contours denote the silhouette boundaries of $\mathcal{M}$ in each image. The bottom row shows the augmented myomas.}
% %	\label{fig:qualResults}
% %\end{figure*}
% More experiments results are provided in the supplemental material (\url{https://bit.ly/2m9rGHH}).

% \subsection{Live usage and quantitative ex-vivo user evaluation with porcine kidneys}
% %\subsection{Overview}
% %\paragraph{Materials.}
%  We used $29$ porcine kidneys recovered from pigs operated after resident training. For each kidney pseudo-tumors were created by injecting alginate, a hardening hydrocolloid, of between \SI{4}{\milli\metre} and \SI{10}{\milli\metre} in diameter.
% In total $59$ pseudo-tumors were injected at arbitrary sub-surface positions, with an average of $2.5$ per kidney.
% We used safe tissue margins of \SI{5}{\milli\metre}.
% Kidney models were made as described in \sect{sec:inputModels} from 3T MRI images (\SI{0.4}{\milli\metre} resolution and slice thickness \SI{1.5}{\milli\metre}).
% The interventional equipment is shown in \fig{fig:visualCompare}\,(c) and consisted of a Karl Storz \SI{10}{\milli\metre} laparoscope column with CLARA image enhancement, a surgical grasper, an incision tool, a laparoscopic pelvic trainer and an instrument with a surgical marker pen attached at the tip (referred to as the \emph{marker instrument}).
% The AR software ran on a mid-range Intel i7 desktop workstation with an NVidia 980 Ti GPU, with visualisations shown on a \SI{26}{\inch} monitor.
% Laparosurgery was performed by a skilled final-year resident.
% The resident spent time training before evaluation to familiarise the task, the guidance software and to provide feedback to improve visualisation.
% In total $28$ pseudo-tumors were resected during this time. 


% \paragraph{AR guidance with Tool Access Visualisation.}
% \label{sec:toolPortProj}

%  Having registered, the final task is AR visualisation.
% We briefly describe \emph{Transparent Blending} (TB) visualisation, which is the previous approach used with monocular laparoscopes.
% It works by first rendering the tumors on the laparoscope's image plane, then a composite image is made by blending the render with the real image to give the impression the organ is transparent.
% An example from~\cite{Collins2044} is shown in \fig{fig:visualCompare}\,(a) where two myomas are visualised with TB.
% TB however has a serious limitation which has not been previously addressed, and we find it can actually \emph{mis-guide the surgeon}.
% The problem is illustrated in \fig{fig:mis-guidance}\,(a) and is as follows.
% When a surgeon actually uses TB to resect a tumor they usually assume it indicates where they should cut to access the tumor.
% This however is incorrect.
% It just shows the position of the tumor from the viewpoint of the laparoscope.
% Often they assume the tumor's centre would be reached by cutting into the organ from the rendered tumor's centre $\mathbf{c}\in\mathbb{R}^2$.
% This is not the case as shown in \fig{fig:mis-guidance}\,(a).
% In our user study we found this is a significant problem with smaller and/or deeper tumors, and can cause them to be missed.
% % increases as the tumor's depth increases and the separation between the tool and laparoscope trocars increases. 
%   \begin{figure*}[h]
%   	\centering
%   	\includegraphics[width=0.9\textwidth]{./figs/kidneyUserStudy/visualisationCompare.pdf}
%   	\caption{(a) AR with Transparent Blending (TB) visualisation taken from~\cite{Collins2044}. (b) Our AR visualisation combining Transparent Blending with Tool Access Visualisation. (c) Our AR system in live operation during the ex-vivo user study.% (c) tumor margins marked on the organ's surface using Tool-port Projection visualisation with a surgical marker. The marks are then used to guide the surgon during tumor resection.
%   	}
%   	\label{fig:visualCompare}
%   \end{figure*}

%  \begin{figure*}%\textbf{}
%  	\centering
%  	\subfloat[AR with Render Blending]{{\includegraphics[width=5.4cm]{./figs/kidneyUserStudy/marginVisual1.pdf} }}%
%  	\qquad
%  	\subfloat[AR with Tool Access Visualisation]{{\includegraphics[width=5.4cm]{./figs/kidneyUserStudy/marginVisual2.pdf} }}%
%  	\caption{The difference between typical AR visualisation of a tumor (a), which does not take into account the position and access direction of the incision tool, and the proposed Tool Access Visualisation (b) which does.}%
%  	\label{fig:mis-guidance}%
%  \end{figure*}
 
% What the surgeon actually wants is to be shown how to reach the tumor using the incision tool.
% Furthermore, surgeons typically want to also see the tumor's safe tissue margin.
% We provide both information with what we call \emph{Tool Access Visualisation}, which is shown in \fig{fig:visualCompare}\,(b).
% Its associated geometry is shown in \fig{fig:mis-guidance}\,(b).
% {Tool Access Visualisation works by showing the tumor's safe tissue margin projected onto the organ's surface as a ring, which we call the \emph{tumor guidance ring}.
% The idea is that if the surgeon were to cut into the organ along the guidance ring, they would segment the tumor with a minimal margin of $w\,$mm.
% At present we do not visualise uncertainty in the margin's location, which is important for real clinical use, and leave this to future work.\textbf{}
  	
% We achieve Tool Access Visualisation with \emph{two} projections.
% The first is a perspective projection of the margin's surface onto the organ's surface, using a centre-of-projection located at the incision tool's port centre $\mathbf{p}\in\mathbb{R}^3$.
% The second is a perspective projection of the projected margin's perimeter onto the laparoscope's optical image (shown as rings in \fig{fig:visualCompare}\,(b)).
% To achieve this we need to know $\mathbf{p}$.
% Recall that the organ has been registered in laparoscope coordinates, therefore we need $\mathbf{p}$ in laparoscope coordinates.
% It may be possible to estimate $\mathbf{p}$ automatically using external and/or internal tool tracking, however this is left to future work.
% Here we assume $\mathbf{p}$ is given \textit{a priori}.
% In our user study, where the ports are located on a pelvic trainer, this is simple and can be done offline by taking physical measurements.
% We complete the visualisation by combining Tool Access Visualisation with TB visualisation (\fig{fig:visualCompare}\,(b))  to show tumors (solid fill), organ (wireframe) and safe tissue margins (wireframe).



% \paragraph{Interventional Protocol and Equipment}
% Laparosurgery was performed using the pelvic trainer, with the kidney inserted on a ground surface and the laparoscope and instruments inserted through three ports.
% The same port configuration was used in all cases.
% The surgeon was tasked to remove each tumor by cutting out a conic tissue section which included the tumor and its safe tissue margin.
% The kidneys were divided into two groups (non-randomised): the \emph{AR group} and the \emph{Non-AR group}, with 13 kidneys in the AR group with 29 tumors, and 19 kidneys in the Non-AR group with 33 tumors.
% Kidneys in the AR group were operated with the AR guidance system activated.
% Recall that the guidance system is not designed to handle significant deformation or topology change after the initial registration, which occurs when a tumor is resected.
% This was dealt with in the protocol by having the surgeon first mark dots along the tumor guidance ring using the marker instrument, guided by the AR visualisation.
% Once completed they used the marks to guide the resection with AR disactivated.
% For the Non-AR group, the surgeon first consulted the MRI using interactive slice-based visualisation~\cite{Wolf_themedical}.
% The task was then performed without AR guidance using the same safe tissue margin of \SI{5}{\milli\metre}.
 
 
% \paragraph{Results}
% We present results with the negative margin rate.
% A negative margin occurs when the tumor is contained entirely within the resected tissue.
% A positive margin occurs when either the tumor is completely absent from the resected tissue (a \emph{complete miss}), or if it is partially contained (a \emph{contact}).
% For three tumors the protocol was not completed properly (the conic section did not cut fully through the kidney) and were excluded.
% There were $13$ negative margins in the Non-AR group (\SI{41.9}{\percent}), with $4$ complete misses and $14$ contacts.
% There were $23$ negative margins in the AR group (\SI{85.2}{\percent}), with $0$ complete misses and $4$ contacts.
% Statistical significance was measured with Fisher's exact two-tailed test with  $p=0.0010$.
% Therefore the user study indicates a very significant benefit for using the AR guidance system.
\subsection{Use with Patients in the Operating Room}\label{subsec:use-with-patients-in-the-operating-room}
We tested our AR system in laparoscopic surgery~\cite{bourdel2017} and report on three patients with one, three and two uterine myomas respectively. 
We built the 3D preoperative models of the organ and the myomas from preoperative T2-weighted MR\@.
% Three-dimensional models of the patient uteri and myomas were constructed before surgery from T2-weighted MR. 
We used our system so that the surgeon could see the location of the myomas in real time. 

In \fig{fig:realOR}(a) we show the MR of the first patient with a $\SI{6}{\centi\metre}$ uterine myoma. \fig{fig:realOR}(b) shows the augmentation of the myoma and \fig{fig:realOR}(c) shows its resection.
At that stage our algorithm shows a past augmentation due to the changes in the uterus surface.
In \fig{fig:realOR}(d) we show our system deployed in the operating room.
The MR of the second patient is shown in \fig{fig:realOR}(e), showing three myomas that are visualized with AR in \fig{fig:realOR}(f,g,h).
The third patient had two myomas whose segmentation from MR is shown in \fig{fig:realOR}(i).
Examples of augmentation of the myomas using bright colors and the uterine cavity mesh are displayed in \fig{fig:realOR}(j,k,m).
We recall that this is the first time one demonstrates markerless registration and AR during \emph{live} laparoscopic surgery. \SG{In our experience both the initial and tracking AR stages are valuable to the surgeon. The first stage is necessary to give the first appreciation of hidden structure locations (tumours and the uterine canal). Recall however that the surgeon is using a monocular camera. The value of the tracking stage is to give the sensation of depth and parallax, and to help orient the uterus to establish a good resection plane. Specifically, the surgeon can move the uterus with the cannula and they receive interactive visual feedback. Depth and spatial comprehension of sub-surface structures are easier because of motion and parallax effects.} 
 

